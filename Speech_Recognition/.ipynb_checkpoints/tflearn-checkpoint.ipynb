{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#Scientific Library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from scipy.misc import imread\n",
    "\n",
    "# Visualization Library\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 classes from the list : \n",
    "yes, no, up, down, left, right, on, off, stop, go\n",
    "\n",
    "2 classes below :\n",
    "1. 6 pictures from silence\n",
    "2. everything else to unknown\n",
    "\n",
    "Take 400 pictures from each class above - 4000 total\n",
    "and from the rest of 20 classes, take 20 pictures from each - 400 pictures for unknown\n",
    "\n",
    "Possible Data Augmentation on 6 pictures for silence\n",
    "\n",
    "-> for now, total of 4000 + 400 + 6 = 4406 pictures.\n",
    "Everything in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = os.listdir('../input/audio/cat')\n",
    "down = os.listdir('../input/audio/down')\n",
    "four = os.listdir('../input/audio/four')\n",
    "house = os.listdir('../input/audio/house')\n",
    "nine = os.listdir('../input/audio/nine')\n",
    "on = os.listdir('../input/audio/on')\n",
    "seven = os.listdir('../input/audio/seven')\n",
    "stop = os.listdir('../input/audio/stop')\n",
    "two = os.listdir('../input/audio/two')\n",
    "yes = os.listdir('../input/audio/yes')\n",
    "bed = os.listdir('../input/audio/bed')\n",
    "eight = os.listdir('../input/audio/eight')\n",
    "go = os.listdir('../input/audio/go')\n",
    "left = os.listdir('../input/audio/left')\n",
    "no = os.listdir('../input/audio/no')\n",
    "one = os.listdir('../input/audio/one')\n",
    "sheila = os.listdir('../input/audio/sheila')\n",
    "three = os.listdir('../input/audio/three')\n",
    "up = os.listdir('../input/audio/up')\n",
    "zero = os.listdir('../input/audio/zero')\n",
    "bird = os.listdir('../input/audio/bird')\n",
    "dog = os.listdir('../input/audio/dog')\n",
    "five = os.listdir('../input/audio/five')\n",
    "happy = os.listdir('../input/audio/happy')\n",
    "marvin = os.listdir('../input/audio/marvin')\n",
    "off = os.listdir('../input/audio/off')\n",
    "right = os.listdir('../input/audio/right')\n",
    "six = os.listdir('../input/audio/six')\n",
    "tree = os.listdir('../input/audio/tree')\n",
    "wow = os.listdir('../input/audio/wow')\n",
    "silence = os.listdir('../input/audio/silence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2377\n"
     ]
    }
   ],
   "source": [
    "print(len(yes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by doing ls -R | wc -l, found out there are total of 64823 spectrogram. We will put in 100 from each first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.empty((100,129,256), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index=0\n",
    "#image_name_list = sheila\n",
    "#for image_name in image_name_list[:100]:\n",
    "#    imageA = plt.imread('../input/audio/sheila/' + image_name)\n",
    "#    data[index] = imageA\n",
    "#    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=P size=50x50 at 0x7F1B90003780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "a=Image.open('../input/audio/sheila/' + sheila[9]).resize((50,50))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Yes Spectrogram Images to Pixels\n",
    "\n",
    "data = np.empty((4800,50,50), dtype=np.float32)\n",
    "\n",
    "# top 10\n",
    "\n",
    "index = 0\n",
    "image_name_list = yes\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/yes/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = no\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/no/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "        \n",
    "image_name_list = up\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/up/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = down\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/down/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = left\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/left/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = right\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/right/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = on\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/on/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = off\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/off/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = stop\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/stop/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = go\n",
    "for image_name in image_name_list[:400]:\n",
    "    imageA = Image.open('../input/audio/go/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "# Top 10 done\n",
    "\n",
    "# Silence 400 pictures\n",
    "    \n",
    "image_name_list = silence\n",
    "for image_name in image_name_list:\n",
    "    imageA = Image.open('../input/audio/silence/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1    \n",
    "\n",
    "# Unknown : 20 pictures from the rest\n",
    "    \n",
    "image_name_list = cat\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/cat/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "     \n",
    "image_name_list = four\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/four/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = house\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/house/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = nine\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/nine/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = seven\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/seven/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = two\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/two/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "\n",
    "image_name_list = bed\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/bed/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = eight\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/eight/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = one\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/one/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = sheila\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/sheila/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = three\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = plt.imread('../input/audio/three/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = zero\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/zero/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = bird\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/bird/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = dog\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/dog/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = five\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/five/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = happy\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/happy/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = marvin\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/marvin/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1 \n",
    "    \n",
    "image_name_list = six\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/six/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = tree\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/tree/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1\n",
    "    \n",
    "image_name_list = wow\n",
    "for image_name in image_name_list[:20]:\n",
    "    imageA = Image.open('../input/audio/wow/' + image_name).resize((50,50))\n",
    "    data[index] = imageA\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 50, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4800 data sets with 30 classes and 100 spectrograms in each class\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.,   4.,   4., ...,  13.,  13.,   4.],\n",
       "       [  8.,   4.,   6., ...,  14.,  17.,   4.],\n",
       "       [ 10.,   4.,  20., ...,   4.,  12.,  16.],\n",
       "       ...,\n",
       "       [ 56.,  59.,  53., ...,  67.,  70.,  54.],\n",
       "       [ 78.,  73.,  79., ...,  82.,  78.,  70.],\n",
       "       [129., 133., 132., ..., 127., 123., 128.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the end is not empty meaning that 3000 has all been transferred\n",
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show one image\n",
    "plt.imshow(data[456])\n",
    "print(data[456].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten\n",
    "# 50, 50 -> 2500\n",
    "#single_image = data[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_image = single_image.reshape(129, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_df = pd.DataFrame(single_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_df = random_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_df[-1] = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min of pixel : 0.015686275\n",
      "max of pixel : 0.7529412\n"
     ]
    }
   ],
   "source": [
    "print(\"min of pixel : \" + str(data[0].min()))\n",
    "print(\"max of pixel : \" + str(data[0].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#concatenate all the flattened spectrograms in order\\nfl_data = np.empty((4800,2500), dtype=np.float32)\\n\\nfor i in range(len(data)):\\n    flat = data[i].flatten()\\n    fl_data[i] = flat\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#concatenate all the flattened spectrograms in order\n",
    "fl_data = np.empty((4800,2500), dtype=np.float32)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    flat = data[i].flatten()\n",
    "    fl_data[i] = flat\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# check if transferred all\\nprint(\"size : \" + str(fl_data.shape))\\nprint(fl_data[-1])\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# check if transferred all\n",
    "print(\"size : \" + str(fl_data.shape))\n",
    "print(fl_data[-1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data = np.empty((4800,2500), dtype=np.float32)\n",
    "for i in range(len(data)):\n",
    "    f_data[i] = data[i].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(30):\n",
    "#    print(i % 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique\n",
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x)\n",
    "    # print list \n",
    "    for x in unique_list: \n",
    "        print (x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "label = np.zeros((400,), dtype=int)\n",
    "\n",
    "for i in range(1, 12):\n",
    "    label = np.concatenate((label, np.repeat(i, 400)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# check uniques\n",
    "unique(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800,)\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(label.shape)\n",
    "print(type(label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = label.reshape((4800,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(np.float64(label[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fl_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#one hot encode the lable\\nb = np.zeros((label.size, label.max()+1))\\nb[np.arange(label.size), label] = 1\\n#labels = b\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#one hot encode the lable\n",
    "b = np.zeros((label.size, label.max()+1))\n",
    "b[np.arange(label.size), label] = 1\n",
    "#labels = b\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check on prepared datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(fl_data.shape)\\nprint(labels.shape)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(fl_data.shape)\n",
    "print(labels.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before dividing into train and test sets, let's combine so we can shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=np.concatenate((fl_data, label), axis=1)\n",
    "#a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "#np.random.shuffle(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to slice the labels again\n",
    "#new_labels=a[:,-1:]#.sum()\n",
    "#new_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to slice the train again\n",
    "#train_data=a[:,:2500]\n",
    "#train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3840, 2500), (960, 2500), (3840,), (960,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(f_data, label, test_size = 0.2)\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_train = train_data[:4000]\\nprint(\"train : \" + str(x_train.shape))\\nx_val = train_data[4000:]\\nprint(\"train validation : \" + str(x_val.shape))\\ny_train = new_labels[:4000]\\ny_train = np.int64(y_train)\\nprint(\"train label : \" + str(y_train.shape))\\ny_val = new_labels[4000:]\\ny_val = np.int64(y_val)\\nprint(\"train validation label : \" + str(y_val.shape))\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x_train = train_data[:4000]\n",
    "print(\"train : \" + str(x_train.shape))\n",
    "x_val = train_data[4000:]\n",
    "print(\"train validation : \" + str(x_val.shape))\n",
    "y_train = new_labels[:4000]\n",
    "y_train = np.int64(y_train)\n",
    "print(\"train label : \" + str(y_train.shape))\n",
    "y_val = new_labels[4000:]\n",
    "y_val = np.int64(y_val)\n",
    "print(\"train validation label : \" + str(y_val.shape))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#change data types to df\n",
    "x_train = pd.DataFrame(x_train)\n",
    "x_val = pd.DataFrame(x_val)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_val = pd.DataFrame(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train[0][0]))\n",
    "y_train = np.int32(y_train)\n",
    "print(type(y_train[0]))\n",
    "print(type(x_val[0][0]))\n",
    "y_val = np.int32(y_val)\n",
    "print(type(y_val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3840, 2500)\n",
      "(3840, 1)\n",
      "(960, 2500)\n",
      "(960, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-49-39b408b40fc2>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-39b408b40fc2>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    model.fit(x_train, y_train, n_epoch=10, validation_set(x_val,y_val), show_metric=True, batch_size=64)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "net=tflearn.input_data([None, 50, 50])\n",
    "net=tflearn.lstm(net,128,dropout=0.5)\n",
    "net=tflearn.fully_connected(net,10,activation='softmax')\n",
    "net=tflearn.regression(net, optimizer='adam', learning_rate=learning_rate,\n",
    "                      loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "while 1:\n",
    "    model.fit(x_train, y_train, n_epoch=10, validation_set(x_val,y_val), show_metric=True, batch_size=64)\n",
    "    _y=model.predict(x_train)\n",
    "model.save('tflearn.lstm.model')\n",
    "print(_y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Helper f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "'''\n",
    "x - according to the documentation I am passing a dictionary of my training features\n",
    "y - passing the training labels\n",
    "batch_size - how many images I want to pass to my NN in a single training step. \n",
    "             The gradients and the loss for the training step will be calculated only on these images. \n",
    "num_epochs - when I want to complete the execution of this method. With the current setting (1) \n",
    "             when the input function goes through all images once it will complete.\n",
    "shuffle    - do I want to read the images in order or no. It is a better strategy to \n",
    "             shuffle within the training images during training.\n",
    "'''\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={'x':x_train},\n",
    "      y=y_train,\n",
    "      batch_size=1,\n",
    "      num_epochs=1,\n",
    "      shuffle=True)\n",
    "\n",
    "'''\n",
    "What the eval_input_fn will do with its current settings is to read the entire training dataset \n",
    "(in order) using batch_size of 128 (default setting). \n",
    "\n",
    "Keep in mind that it will also terminate when completes 1 epoch.\n",
    "'''\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x':x_val},\n",
    "    y=y_val,\n",
    "    shuffle=False,\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_function(features, labels, mode):\n",
    "    '''\n",
    "    This is the function which describes the structure of the neural network\n",
    "    '''\n",
    "\n",
    "    layer_1 = tf.reshape(features['x'], [-1, 50, 50, 1])\n",
    "    \n",
    "\n",
    "    layer_2 = tf.layers.conv2d(                          \n",
    "                inputs=layer_1,\n",
    "                filters=10,\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "    \"\"\"\n",
    "    layer_2 = tf.layers.batch_normalization(layer_2)\n",
    "\n",
    "    layer_3 = tf.layers.conv2d(\n",
    "                inputs=layer_2,\n",
    "                filters=10,\n",
    "                kernel_size=[4, 4],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "    \n",
    "    layer_3 = tf.layers.batch_normalization(layer_3)\n",
    "    \"\"\"\n",
    "    \n",
    "    layer_4 = tf.reshape(layer_2, [-1, 50 * 50 * 10])\n",
    "    \n",
    "\n",
    "    layer_5 = tf.layers.dense(inputs=layer_4, units=100, activation=tf.nn.relu)\n",
    "    \n",
    "\n",
    "    layer_6 = tf.layers.dropout(inputs=layer_5, rate=0.5, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "    \n",
    "\n",
    "    logits = tf.layers.dense(inputs=layer_6, units=12)\n",
    "    \n",
    "\n",
    "    predictions = {\n",
    "                \"classes\": tf.argmax(input=logits, axis=1),\n",
    "                \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions['classes'], name='acc_op')\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        tf.identity(accuracy[1], name='train_accuracy')\n",
    "        tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "        eval_metric_ops = {'train_accuracy':accuracy}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)\n",
    "    \n",
    "    tf.identity(accuracy[1], name='val_accuracy')\n",
    "    tf.summary.scalar('val_accuracy', accuracy[1])\n",
    "    eval_metric_ops = {'val_accuracy':accuracy}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "                mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './CNN_CLASSIFIER', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe4b47f0da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# first we define a folder where tensorflow will keep its progress\n",
    "# this includes periodical saves of our weight, biases, accuracy, loss and etc.\n",
    "# so if we have more training images we can simply continue training on them\n",
    "OUTDIR = './CNN_CLASSIFIER'\n",
    "\n",
    "# we create an estimator object which:\n",
    "# - is using the Neural Net structure from the cnn_model_function\n",
    "# - reads/writes the files written in the directory which we defined earlier\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_function, model_dir=OUTDIR)\n",
    "\n",
    "# here we start the FileWriter method which will actually save the progress in the folder defined above\n",
    "file_writer = tf.summary.FileWriter(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./CNN_CLASSIFIER/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./CNN_CLASSIFIER/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3106961, step = 1\n",
      "INFO:tensorflow:global_step/sec: 67.8928\n",
      "INFO:tensorflow:loss = 2.2738838, step = 101 (1.475 sec)\n",
      "ERROR:tensorflow:Model diverged with loss = NaN.\n"
     ]
    },
    {
     "ename": "NanLossDuringTrainingError",
     "evalue": "NaN loss during training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNanLossDuringTrainingError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-049fcabaab4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# and here we finally start training/evaluating the NN for 30 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-049fcabaab4e>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1179\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1213\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1214\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1149\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1222\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1302\u001b[0m               \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m               run_metadata=run_metadata))\n\u001b[0m\u001b[1;32m   1305\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mafter_run\u001b[0;34m(self, run_context, run_values)\u001b[0m\n\u001b[1;32m    750\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fail_on_nan_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailure_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNanLossDuringTrainingError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailure_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNanLossDuringTrainingError\u001b[0m: NaN loss during training."
     ]
    }
   ],
   "source": [
    "# a small helper function which trains/evaluates our network for a given number of epochs\n",
    "# remember that our input functions go through the datasets only once\n",
    "def train_and_evaluate(estimator, epochs=3):\n",
    "    for i in range(epochs):\n",
    "        estimator.train(input_fn=train_input_fn)\n",
    "        estimator.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "# and here we finally start training/evaluating the NN for 30 epochs\n",
    "train_and_evaluate(cnn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization function\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return (tf.Variable(init_random_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias initialization function\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d convolution function which is already performed by tf\n",
    "def conv2d(x, W):\n",
    "    # x => input tensor ==> [batch, H, W, Channels]\n",
    "    # W => kernel => [filter height, filter width, # of channels in, # channels out]\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1],padding=\"SAME\")\n",
    "# padding with SAME adds zeros to the end\n",
    "# Strides is how you want to move in the whole thing [batch, height, width, channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling function in this case max pooling which gets the max value and is a 2x2 kernel\n",
    "def max_pool_2by2(x):\n",
    "    # x=> input => [batch, h, w, c]\n",
    "    return (tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"))\n",
    "# Ksize = size of the window which makces the pooling [batch, h, w, c] => here 1 is like pasing all 2 is reducing\n",
    "    # We only want to reduce the height and width of the image, that is why we use \n",
    "    # [1 in batch, 2 in height, 2 in width, 1 in channel]\n",
    "#  Stride here is [1, 2, 2, 1] because we want to shorten the image so we jump 2 by 2 pixels in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layer function\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]]) \n",
    "    # we take the third one because that is the number of channels we are using\n",
    "    # is not really intuitive but the channels is like the number of features\n",
    "    # first is going to be 1 then more\n",
    "    return (tf.nn.relu(conv2d(input_x, W)+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal layaer (fully connected layer)\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1]) \n",
    "    # we use the index one beacause 0 is the batch size and 1 is the number of nodes\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return (tf.matmul(input_layer, W)+b)\n",
    "# simple weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 50 * 50])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 50, 50, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_1 = convolutional_layer(x_image, shape=[5,5,1,40])\n",
    "convo_1_pooling = max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_2 = convolutional_layer(convo_1_pooling, shape=[5,5,40,20])\n",
    "convo_2_pooling = max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_3 = convolutional_layer(convo_2_pooling, shape=[5,5,20,10])\n",
    "convo_3_pooling = max_pool_2by2(convo_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_2_flat = tf.reshape(convo_3_pooling,[-1, 490])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = normal_full_layer(full_one_dropout, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savers\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summaries for tensorboard\n",
    "merged_summary_op = tf.summary.merge_all() # this will get all the summaries from the graph and join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-399ea7f78781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrand_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Here we add hold_prob to our feed dictionary because it is a placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "steps = 2000\n",
    "batch_size = 12\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # to use tensorboard you need to have a file writer to keep the logs\n",
    "    train_writer = tf.summary.FileWriter(\"./tensorboard/train\", graph=tf.get_default_graph())\n",
    "    validation_writer = tf.summary.FileWriter(\"./tensorboard/validation\", graph=tf.get_default_graph())\n",
    "    \n",
    "    for i in range(steps + 1):\n",
    "        \n",
    "        rand_ind = np.random.randint(len(x_train), size=batch_size)\n",
    "        # Here we add hold_prob to our feed dictionary because it is a placeholder\n",
    "        feed = {x:x_train.loc[rand_ind], y_true:y_train.loc[rand_ind], hold_prob:0.6}\n",
    "        sess.run(train, feed_dict=feed)\n",
    "        \n",
    "        \n",
    "        # When runnining you need to append logs for tensorboard\n",
    "        if i%100 == 0:\n",
    "            summary = sess.run(merged_summary_op,\n",
    "                                      {x:x_train.loc[rand_ind], y_true:y_train.loc[rand_ind], hold_prob:1.0})\n",
    "            train_writer.add_summary(summary, i)\n",
    "            train_writer.flush()\n",
    "            summary = sess.run(merged_summary_op,\n",
    "                                      {x:x_val, y_true:y_val, hold_prob:1.0})\n",
    "            validation_writer.add_summary(summary, i)\n",
    "            validation_writer.flush()\n",
    "            print(\"tensorboard summary at #{}\".format(i))\n",
    "    \n",
    "    saver.save(sess, \"cnn_model/cnn.ckpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
